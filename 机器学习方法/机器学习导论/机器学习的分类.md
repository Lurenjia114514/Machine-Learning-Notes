# 机器学习的分类

## 基本分类

### 监督学习

监督学习是指从标注数据中学习预测模型的机器学习问题. 标注数据表示输入输出的映射关系, 预测模型则将给定的输入映射为相应的输出. 若输出为连续变量, 则该问题称为回归; 若输出为分类变量, 则问题称为分类.

我们称输入和输出所有可能的取值集合分别为输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$. 我们将输入 $\boldsymbol{x}$ 和输出 $\boldsymbol{y}$ 定义为在输入和输出空间上的随机变量的取值. 其中输入变量记为 $X$, 输出变量为 $Y$. 例如输入向量为
$$
\boldsymbol{x}=
\begin{bmatrix}
x^{(1)} \\ \cdots \\ x^{(n)}
\end{bmatrix}
$$

训练集和测试集由样本(即输入输出对)组成, 表示为
$$
T=\{(\boldsymbol{x}_1,\boldsymbol{y}_1), \cdots ,(\boldsymbol{x}_N,\boldsymbol{y}_N)\}
$$

监督学习假设输入和输出变量 $X$ 和 $Y$ 服从联合分布 $P(X, Y)$, 且训练集和测试集是根据联合分布 $P(X,Y)$ 独立同分布产生的.

监督学习的目的为从假设空间中学习一个从输入到输出的最优的映射模型. 监督学习的模型可以是属于概率模型的条件概率分布 $P(Y|X)$ 和 属于非概率模型的决策函数 $Y=f(X)$.

在预测过程中, 对于测试集中的 $\boldsymbol{x}_{N+1}$ , 根据模型
$$
\boldsymbol{y}_{N+1} = \arg \max_{\boldsymbol{y}} \hat{P}(\boldsymbol{y}|\boldsymbol{x}_{N+1})
$$
或
$$
\boldsymbol{y}_{N+1} = \arg \max_{\boldsymbol{y}} \hat{f}(\boldsymbol{x}_{N+1})
$$
给出相应的输出 $\boldsymbol{y}_{N+1}$

一般地, 对于训练集中的输入 $\boldsymbol{x}_{i}$ 有输出 $\boldsymbol{y}_{i}$, 而模型 $y=f(x)$ 则输出 $f(\boldsymbol{x}_{i})$ , 学习系统则通过不断尝试降低训练样本输出 $\boldsymbol{y}_{i}$ 和模型输出 $f(\boldsymbol{x}_{i})$ 的差, 选出具有最强预测能力的模型.

### 无监督学习

无监督学习是指从无标注数据中学习预测模型的机器学习问题, 其中模型表示数据的类别, 转换或者概率, 其本质为学习数据中的统计规律或者潜在结构.

无监督学习的训练集表示为
$$
U=\{\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N\}
$$

设 $\mathcal{X}$ 为输入空间, $\mathcal{Z}$ 为隐式结构空间, 学习模型可表示为函数 $\boldsymbol{z}=g(\boldsymbol{x})$ , 条件分布 $P(\boldsymbol{z}|\boldsymbol{x})$ 和 $P(\boldsymbol{x}|\boldsymbol{z})$. 其中前两个模型可进行聚类和降维, 后一个可进行概率估计.

### 强化学习

强化学习是指是智能体通过与环境不断交互, 从反馈信号中学习如何在不同情境下采取行动以最大化长期收益.

### 半监督学习和主动学习

半监督学习是指利用未标注数据中的信息, 辅助标注学习, 进行监督学习,学习预测模型的机器学习问题.

主动学习是指计算机不断主动给出实例让教师进行标注, 然后利用标注数据学习预测模型的机器学习问题.

![1](Image\317d7365991af82936f32a9bd1731501.png)

## 按模型分类

### 概率模型和非概率模型

机器学习按照模型可分为概率模型和非概率模型. 其中概率模型取条件分布 $P(\boldsymbol{y}|\boldsymbol{x})$ 的形式, 而非概率模型取函数 $\boldsymbol{y}=f(\boldsymbol{x})$ 的形式.

条件概率分布和决策函数可相互转换: 条件概率分布最大化后可得到决策函数, 决策函数归一化后可得到条件概率分布. 概率模型与非概率模型最大的不同之处在于概率模型可表示为联合分布的形式, 而非概率模型则不能.

常用的概率公式有
$$
\begin{align}
\text{加法公式}~~~ P(\boldsymbol{x})&=\sum_{\boldsymbol{y}}P(\boldsymbol{x},\boldsymbol{y}) \\
\text{乘法公式} ~~~P(\boldsymbol{x},\boldsymbol{y})&=
P(\boldsymbol{y}|\boldsymbol{x})P(\boldsymbol{x})
\end{align}
$$

常见的概率模型有决策树, 朴素贝叶斯, 隐马尔可夫模型, 条件随机场, 概率潜在语义分析, 潜在狄利克雷分配和高斯混合模型等. 常见的非概率模型有感知机, 支持向量机, $K$ 邻近, AdaBoost, $K$ 均值, 潜在语义分析和神经网络等. 逻辑回归即可视为概率模型, 又可视为非概率模型.

概率模型的代表为概率图模型, 它的联合分布由有向图或无向图表示, 且可根据图的结构分解为因子乘积的形式. 常见的概率图模型有贝叶斯网络, 马尔可夫随机场和条件随机场等.

### 线性模型和非线性模型

对于非概率模型, 若决策函数为线性函数, 则称为线性模型, 否则称为非线性模型.

常见的线性模型有感知机, 线性支持向量机, $K$ 邻近, $K$ 均值和潜在语义分析等. 常见的非线性模型有核函数支持向量机, AdaBoost和神经网络等.

### 参数模型和非参数模型

若模型的参数为有限维的, 则称为参数化模型; 若参数的维度不固定或者无限维, 则称为非参数化模型.

常见的参数化模型有感知机, 朴素贝叶斯, 逻辑回归, $K$ 均值, 高斯混合模型, 潜在语义分析, 概率潜在语义分析和潜在狄利克雷分配等. 常见的非参数化模型有决策树, 支持向量机, AdaBoost和 $K$ 邻近等.

![1](Image\31b52fa03b9f789fb62e78bbe52c2049.png)

## 按技巧分类

### 贝叶斯学习

贝叶斯学习是指我们利用贝叶斯定理计算在给定数据下的模型的条件概率, 即后验概率, 并进行估计和预测. 其中将模型, 未观测要素和参数用变量表示. 常见的贝叶斯学习有朴素贝叶斯和潜在狄利克雷分配.

设随机变量 $D$ 表示数据, 随机变量 $\theta$ 表示模型参数, 有
$$
P(\theta|D)=\frac{P(D|\theta)}{P(D)}P(\theta)
$$

若给出一个最优模型时, 我们通常最大化后验概率.

在进行预测时, 我们计算数据对后验概率分布的期望值
$$
P(x|D)=\int P(x|\theta, D)P(\theta|D)\mathrm{d}\theta
$$

### 核方法

设输入空间到特征空间的映射为 $\varphi$ , 核方法通过在输入空间中定义核函数 $K(\boldsymbol{x}_1,\boldsymbol{x}_2)=<\varphi(\boldsymbol{x}_1),\varphi(\boldsymbol{x}_2)>$ , 简化了映射后在特征空间的内积计算.

常见的核方法模型有核函数支持向量机, 核 $PCA$ 和核 $k$ 均值等.

![1](Image\71e67594fbb929f7e3823ec018b673a4.png)

## 按算法分类

机器学习按照算法分类可分为在线学习和批量学习.

在线学习是指每次接受一个样本, 学习模型并进行预测, 并不断重复上述操作的机器学习.

批量学习是指一次接受所有数据后学习模型并预测.
